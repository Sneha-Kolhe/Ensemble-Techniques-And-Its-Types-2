{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a8ee4e1-755b-412e-a85e-dff0859fdc9b",
   "metadata": {},
   "source": [
    "## Q1. How does bagging reduce overfitting in decision trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144b74a1-43b4-4a81-b22b-6bb3a0770b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "Training on Different Subsets:\n",
    "\n",
    "Bagging involves sampling subsets of the training data with replacement to create multiple bootstrap samples.\n",
    "Each base model (decision tree) is trained independently on one of these bootstrap samples.\n",
    "Since each decision tree is trained on a different subset of the data, they are likely to capture different patterns or noise present in the data.\n",
    "Reduced Variance:\n",
    "\n",
    "By averaging or combining the predictions of multiple decision trees trained on different subsets of data, bagging reduces the variance of the overall model.\n",
    "Variance refers to the sensitivity of the model's predictions to fluctuations in the training data.\n",
    "By averaging the predictions of multiple trees, bagging smooths out the predictions and reduces the variability that may arise from individual trees overfitting to specific patterns or noise in the data.\n",
    "Model Aggregation:\n",
    "\n",
    "Bagging combines the predictions of multiple base models using averaging (for regression) or voting (for classification).\n",
    "Since each decision tree is trained independently, they may make different errors or predictions on different subsets of data.\n",
    "By aggregating the predictions of multiple trees, bagging helps reduce the impact of individual errors or biases, leading to a more robust and generalized model.\n",
    "Out-of-Bag (OOB) Error Estimation:\n",
    "\n",
    "In bagging, each base model is trained on a bootstrap sample, leaving out some data points (out-of-bag samples) that were not included in the training.\n",
    "The out-of-bag samples can be used to estimate the model's performance (out-of-bag error) without the need for a separate validation set.\n",
    "This allows for an unbiased estimate of the model's generalization performance and helps prevent overfitting by providing a reliable measure of model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2742575e-af08-4a42-9531-8da87654d17c",
   "metadata": {},
   "source": [
    "## Q2. What are the advantages and disadvantages of using different types of base learners in bagging?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2d02ea-bc17-4dec-9537-3a56819d91dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
